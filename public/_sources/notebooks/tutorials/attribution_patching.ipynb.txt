{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `nnsight` for the [Attribution Patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching) technique.\n",
    "\n",
    "First lets do our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "import einops\n",
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"gpt2\", device_map=\"cuda:0\", dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"When John and Mary went to the shops, John gave the bag to\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
    "    \"When Tom and James went to the park, James gave the ball to\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
    "]\n",
    "answers = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" John\", \" Mary\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" James\", \" Tom\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Sid\", \" Dan\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "    (\" Amy\", \" Martin\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.tokenizer(prompts, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
    "]\n",
    "\n",
    "answer_token_indices = torch.tensor(\n",
    "    [\n",
    "        [model.tokenizer(answers[i][j])[\"input_ids\"][0] for j in range(2)]\n",
    "        for i in range(len(answers))\n",
    "    ],\n",
    "    device=model.local_model.device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_metric(\n",
    "    logits,\n",
    "    CLEAN_BASELINE,\n",
    "    CORRUPTED_BASELINE,\n",
    "    answer_token_indices=answer_token_indices,\n",
    "):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "\n",
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    if len(logits.shape) == 3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.forward(inference=False) as runner:\n",
    "    with runner.invoke(clean_tokens) as invoker:\n",
    "        clean_logits = model.lm_head.output\n",
    "\n",
    "        clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "\n",
    "        clean_cache = [\n",
    "            model.transformer.h[i].attn.attn_dropout.input.save()\n",
    "            for i in range(len(model.transformer.h))\n",
    "        ]\n",
    "\n",
    "        clean_grad_cache = [\n",
    "            model.transformer.h[i].attn.attn_dropout.backward_input.save()\n",
    "            for i in range(len(model.transformer.h))\n",
    "        ]\n",
    "\n",
    "    with runner.invoke(corrupted_tokens) as invoker:\n",
    "        corrupted_logits = model.lm_head.output\n",
    "\n",
    "        corrupted_logit_diff = get_logit_diff(\n",
    "            corrupted_logits, answer_token_indices\n",
    "        ).item()\n",
    "\n",
    "        corrupted_cache = [\n",
    "            model.transformer.h[i].attn.attn_dropout.input.save()\n",
    "            for i in range(len(model.transformer.h))\n",
    "        ]\n",
    "\n",
    "        corrupted_grad_cache = [\n",
    "            model.transformer.h[i].attn.attn_dropout.backward_input.save()\n",
    "            for i in range(len(model.transformer.h))\n",
    "        ]\n",
    "\n",
    "        clean_value = ioi_metric(\n",
    "            clean_logits, clean_logit_diff, corrupted_logit_diff\n",
    "        ).save()\n",
    "\n",
    "        corrupted_value = ioi_metric(\n",
    "            corrupted_logits, clean_logit_diff, corrupted_logit_diff\n",
    "        ).save()\n",
    "\n",
    "        (corrupted_value + clean_value).backward()\n",
    "\n",
    "clean_cache = torch.stack([value.value[0] for value in clean_cache])\n",
    "clean_grad_cache = torch.stack([value.value[0] for value in clean_grad_cache])\n",
    "corrupted_cache = torch.stack([value.value[0] for value in corrupted_cache])\n",
    "corrupted_grad_cache = torch.stack([value.value[0] for value in corrupted_grad_cache])\n",
    "\n",
    "print(\"Clean Value:\", clean_value.value.item())\n",
    "print(\"Corrupted Value:\", corrupted_value.value.item())\n",
    "\n",
    "\n",
    "def create_attention_attr(clean_cache, clean_grad_cache):\n",
    "    attention_attr = clean_grad_cache * clean_cache\n",
    "    attention_attr = einops.rearrange(\n",
    "        attention_attr,\n",
    "        \"layer batch head_index dest src -> batch layer head_index dest src\",\n",
    "    )\n",
    "    return attention_attr\n",
    "\n",
    "\n",
    "attention_attr = create_attention_attr(clean_cache, clean_grad_cache)\n",
    "\n",
    "n_layers = len(model.transformer.h)\n",
    "n_heads = model.transformer.h[0].attn.num_heads\n",
    "\n",
    "HEAD_NAMES = [f\"L{l}H{h}\" for l in range(n_layers) for h in range(n_heads)]\n",
    "HEAD_NAMES_SIGNED = [f\"{name}{sign}\" for name in HEAD_NAMES for sign in [\"+\", \"-\"]]\n",
    "HEAD_NAMES_QKV = [\n",
    "    f\"{name}{act_name}\" for name in HEAD_NAMES for act_name in [\"Q\", \"K\", \"V\"]\n",
    "]\n",
    "print(HEAD_NAMES[:5])\n",
    "print(HEAD_NAMES_SIGNED[:5])\n",
    "print(HEAD_NAMES_QKV[:5])\n",
    "\n",
    "\n",
    "def plot_attention_attr(attention_attr, tokens, top_k=20, index=0, title=\"\"):\n",
    "    if len(tokens.shape) == 2:\n",
    "        tokens = tokens[index]\n",
    "    if len(attention_attr.shape) == 5:\n",
    "        attention_attr = attention_attr[index]\n",
    "    attention_attr_pos = attention_attr.clamp(min=-1e-5)\n",
    "    attention_attr_neg = -attention_attr.clamp(max=1e-5)\n",
    "    attention_attr_signed = torch.stack([attention_attr_pos, attention_attr_neg], dim=0)\n",
    "    attention_attr_signed = einops.rearrange(\n",
    "        attention_attr_signed,\n",
    "        \"sign layer head_index dest src -> (layer head_index sign) dest src\",\n",
    "    )\n",
    "    attention_attr_signed = attention_attr_signed / attention_attr_signed.max()\n",
    "    attention_attr_indices = (\n",
    "        attention_attr_signed.max(-1).values.max(-1).values.argsort(descending=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    attention_attr_signed = attention_attr_signed[attention_attr_indices, :, :]\n",
    "    head_labels = [HEAD_NAMES_SIGNED[i.item()] for i in attention_attr_indices]\n",
    "\n",
    "    tokens = [model.tokenizer.decode(token) for token in tokens]\n",
    "\n",
    "    return cv.circuitsvis.attention.attention_heads(\n",
    "        tokens=tokens,\n",
    "        attention=attention_attr_signed[:top_k],\n",
    "        attention_head_names=head_labels[:top_k],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_attr(\n",
    "    attention_attr,\n",
    "    clean_tokens,\n",
    "    index=0,\n",
    "    title=\"Attention Attribution for first sequence\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
