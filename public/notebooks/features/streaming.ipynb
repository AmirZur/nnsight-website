{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming enables users apply functions and datasets locally during remote model execution. This allows users to stream results for immediate consumption (i.e., seeing tokens as they are generated) or applying non-whitelisted functions such as model tokenizers, large local datasets, and more!\n",
    "\n",
    "*   `nnsight.local()` context sends values immediately to user's local machine from server\n",
    "*   Intervention graph is executed locally on downstream nodes\n",
    "*   Exiting local context uploads data back to server\n",
    "*   `@nnsight.trace` function decorator enables custom functions to be added to intervention graph when using `nnsight.local()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in Google Colab, install nnsight\n",
    "try:\n",
    "    import google.colab\n",
    "    is_colab = True\n",
    "except ImportError:\n",
    "    is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    !pip install -U nnsight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nnsight.local()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may sometimes want to locally access and manipulate values during remote execution. Using `.local()` on a proxy, you can send remote content to your local machine and apply local functions. The intervention graph is then executed locally on downstream nodes (until you send execution back to the remote server by exiting the `.local()` context).\n",
    "\n",
    "There are a few use cases for streaming with `.local()`, including live chat generation and applying large datasets or non-whitelisted local functions to the intervention graph.\n",
    "\n",
    "Now let's explore how streaming works. We'll start by grabbing some hidden states of the model and printing their value using `tracer.log()`. Without calling `nnsight.local()`, these operations will all occur remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import CONFIG\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if is_colab:\n",
    "    # include your HuggingFace Token and NNsight API key on Colab secrets\n",
    "    from google.colab import userdata\n",
    "    NDIF_API = userdata.get('NDIF_API')\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "    CONFIG.set_default_api_key(NDIF_API)\n",
    "    !huggingface-cli login -token HF_TOKEN\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "llama = LanguageModel(\"meta-llama/Meta-Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give you a remote LOG response because it's coming from the remote server\n",
    "with llama.trace(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    hs = llama.model.layers[-1].output[0]\n",
    "\n",
    "    tracer.log(hs[0,0,0])\n",
    "\n",
    "    out =  llama.lm_head.output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight\n",
    "# This will print locally because it's already local\n",
    "with llama.trace(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    with nnsight.local():\n",
    "        hs = llama.model.layers[-1].output[0]\n",
    "        tracer.log(hs[0,0,0])\n",
    "\n",
    "    out =  llama.lm_head.output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@nnsight.trace` function decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use function decorators to create custom functions to be used during `.local` calls. This is a handy way to enable live streaming of a chat or to train probing classifiers on model hidden states.\n",
    "\n",
    "Let's try out `@nnsight.trace` and `nnsight.local()` to access a custom function during remote execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's define our function\n",
    "@nnsight.trace # decorator that enables this function to be added to the intervention graph\n",
    "def my_local_fn(value):\n",
    "    return value * 0\n",
    "\n",
    "# We use a local function to ablate some hidden states\n",
    "# This downloads the data for the .local context, and then uploads it back to set the value.\n",
    "with llama.generate(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    hs = llama.model.layers[-1].output[0]\n",
    "\n",
    "    with nnsight.local():\n",
    "\n",
    "        hs = my_local_fn(hs)\n",
    "\n",
    "    llama.model.layers[-1].output[0][:] = hs\n",
    "\n",
    "    out =  llama.lm_head.output.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that without calling `.local`, the remote API does not know about `my_local_fn` and will throw a whitelist error. A whitelist error occurs because you are being allowed access to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with llama.trace(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    hs = llama.model.layers[-1].output[0]\n",
    "\n",
    "    hs = my_local_fn(hs) # no .local - will cause an error\n",
    "\n",
    "    llama.model.layers[-1].output[0][:] = hs * 2\n",
    "\n",
    "    out =  llama.lm_head.output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Live-streaming remote chat\n",
    "\n",
    "Now that we can access data within the tracing context on our local computer, we can apply non-whitelisted functions, such as the model's tokenizer, within our tracing context.\n",
    "\n",
    "Let's build a decoding function that will decode tokens into words and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnsight.trace\n",
    "def my_decoding_function(tokens, model, max_length=80, state=None):\n",
    "    # Initialize state if not provided\n",
    "    if state is None:\n",
    "        state = {'current_line': '', 'current_line_length': 0}\n",
    "\n",
    "    token = tokens[-1] # only use last token\n",
    "\n",
    "    # Decode the token\n",
    "    decoded_token = llama.tokenizer.decode(token).encode(\"unicode_escape\").decode()\n",
    "\n",
    "    if decoded_token == '\\\\n':  # Handle explicit newline tokens\n",
    "        # Print the current line and reset state\n",
    "        print('',flush=True)\n",
    "        state['current_line'] = ''\n",
    "        state['current_line_length'] = 0\n",
    "    else:\n",
    "        # Check if adding the token would exceed the max length\n",
    "        if state['current_line_length'] + len(decoded_token) > max_length:\n",
    "            print('',flush=True)\n",
    "            state['current_line'] = decoded_token  # Start a new line with the current token\n",
    "            state['current_line_length'] = len(decoded_token)\n",
    "            print(state['current_line'], flush=True, end=\"\")  # Print the current line\n",
    "        else:\n",
    "            # Add a space if the line isn't empty and append the token\n",
    "            if state['current_line']:\n",
    "                state['current_line'] += decoded_token\n",
    "            else:\n",
    "                state['current_line'] = decoded_token\n",
    "            state['current_line_length'] += len(decoded_token)\n",
    "            print(state['current_line'], flush=True, end=\"\")  # Print the current line\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can decode and print our model outputs throughout token generation by accessing our decoding function through `nnsight.local()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "nnsight.CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "prompt = \"A press release is an official statement delivered to members of the news media for the purpose of\"\n",
    "# prompt = \"Your favorite board game is\"\n",
    "\n",
    "print(\"Prompt: \",prompt,'\\n', end =\"\")\n",
    "\n",
    "# Initialize the state for decoding\n",
    "state = {'current_line': '', 'current_line_length': 0}\n",
    "\n",
    "with llama.generate(prompt, remote=True, max_new_tokens = 50) as generator:\n",
    "    # Call .all() to apply to each new token\n",
    "    llama.all()\n",
    "\n",
    "    all_tokens = nnsight.list().save()\n",
    "\n",
    "    # Access model output\n",
    "    out = llama.lm_head.output.save()\n",
    "\n",
    "    # Apply softmax to obtain probabilities and save the result\n",
    "    probs = torch.nn.functional.softmax(out, dim=-1)\n",
    "    max_probs = torch.max(probs, dim=-1)\n",
    "    tokens = max_probs.indices.cpu().tolist()\n",
    "    all_tokens.append(tokens[0]).save()\n",
    "\n",
    "    with nnsight.local():\n",
    "        state = my_decoding_function(tokens[0], llama, max_length=20, state=state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsight_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
